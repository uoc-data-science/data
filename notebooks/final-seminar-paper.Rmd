---
title: "Data analysis: ebuy's sales data"
bibliography: "bibliographies/literature.bib"
output:
  bookdown::html_document2:
    toc: yes
    toc_depth: 2
    theme: spacelab
---
```{r setRngSeed, echo=FALSE}
# To always get (and report) the same cross-validation results (and any
# other random-based things) we initialize the RNG with a fixed seed:
set.seed(26769)
```

# Data Manipulation
```{r includes01, echo=FALSE, message=FALSE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(knitr)
library(kableExtra)
library(usmap)
library(ineq)
library(janitor)
library(stringr)
library(ineq)
```

## Loading the Data
Ebuy's data came separated into different files: files that contained the acutal observations and files that described the columns. We first joined the column names with their respective data sets.
```{r loadingTheData, echo=FALSE, message=FALSE}
# This code chunk used to reside in 'script_02_join_data_with_column_names.r'! Its now part of the "big markdown file".
readdatawithcolumns <- function(headersFile, dataFile) {
    message("Reading headers...")
    tmpHeaderNames <- read.csv(file=headersFile, sep=":", header=FALSE)
    headerNames <- tmpHeaderNames$V1

    message("Replacing spaces...")
    headerNames <- gsub(" ", "_", headerNames)

    message("Adding headers to data file...")
    df <- read.csv(file=dataFile, sep=",", header=FALSE)
    colnames(df) <- headerNames
    return(df)
}

# read the ORDER data:
orderDf <- readdatawithcolumns(headersFile="../data/raw/orders/order_columns.txt",
                               dataFile="../data/raw/orders/order_data.csv")
#write.csv(orderDf, file="data/interim/orders/orders_with_headers.csv", row.names=FALSE)
message("Done!")

# read the CLICKSTREAM:
clickstreamDf1 <- readdatawithcolumns(headersFile="../data/raw/clickstream/clickstream_columns.txt",
                                      dataFile="../data/interim/clickstream/clickstream_data.csv")
clickstreamDf2 <- readdatawithcolumns(headersFile="../data/raw/clickstream/clickstream_columns.txt",
                                      dataFile="../data/raw/clickstream/clickstream_data_part_2.csv")
clickstreamDf <- rbind(clickstreamDf1, clickstreamDf2)
#write.csv(clickstreamDf, file="data/interim/clickstream/clickstream_with_headers.csv", row.names=FALSE)
message("Done!")
```

## Cleaning the Data
After examining the data sets we than proceeded to clean them.
```{r cleaningTheData, echo=FALSE, message=FALSE}
# This code chunk used to reside in 'script_03_clean_data.r'! Its now part of the "big markdown file".
cleanFile <- function(filePath, threshold) {
  message(sprintf("Loading file from %s.", filePath))
  df <- read.csv(file=filePath, header=TRUE)
  # TODO: why does ', na.strings=c("?","NA", "NULL")' not work here?

  message("Cleaning data.")
  message("1. Removing empty rows and columns.")
  dfCleaned <- remove_empty(dat = df, which = c("rows", "cols"))
  columnsDeleted <- ncol(df) - ncol(dfCleaned)
  message(sprintf("1. Removed %d columns", columnsDeleted))

  message("2. Removing constant columns.")
  dfCleaned <- remove_constant(dat = dfCleaned, na.rm = TRUE)
  columnsDeleted <- ncol(df) - ncol(dfCleaned) - columnsDeleted
  message(sprintf("2. Removed %d columns", columnsDeleted))

  message(sprintf("3. Removing columns with unknown values (threshold = %f).", threshold))
  deletableColumns <- list()
  for(c in 1:ncol(dfCleaned)){
    currentColumn <- dfCleaned[,c]
    unknownPercentage <- sum(currentColumn == "NULL" | currentColumn == "?")/length(currentColumn)
    if(unknownPercentage > threshold){
      deletableColumns <- c(deletableColumns, colnames(dfCleaned)[c])
    }
  }

  columnsDeleted <- length(deletableColumns)
  message(sprintf("3. Removed %d columns", columnsDeleted))

  dfCleaned <- dfCleaned[, !(names(dfCleaned) %in% deletableColumns)]

  message(sprintf("File cleaned. Total deleted columns: %d", ncol(df)-ncol(dfCleaned)))
  return(dfCleaned)
}

clickstream_data <- cleanFile("../data/interim/clickstream/clickstream_with_headers.csv", 0.9)
orderDfCleaned <- cleanFile("../data/interim/orders/orders_with_headers.csv", 0.9)
```


## Recoding Variables
We first loaded the data, picked the columns that we were interested in, and forced some of the variables to be categorical.
```{r pickColumnsForceCat01, echo=FALSE}
partialOrdersDf <- orderDfCleaned[c("Product_Family_ID",
                             "Order_Line_Day_of_Week",
                             "Order_Line_Hour_of_Day",
                             "Order_Line_Amount",
                             "Product_ID",
                             "Gender",
                             "US_State",
                             "Age",
                             "Order_Day_of_Week",
                             "BrandName",
                             "Order_Amount")]

partialOrdersDf$Product_Family_ID <- as.factor(partialOrdersDf$Product_Family_ID)
partialOrdersDf$Order_Line_Day_of_Week <- as.factor(partialOrdersDf$Order_Line_Day_of_Week)
partialOrdersDf$Order_Line_Hour_of_Day <- as.factor(partialOrdersDf$Order_Line_Hour_of_Day)
partialOrdersDf$Gender <- as.factor(partialOrdersDf$Gender)
partialOrdersDf$US_State <- as.factor(partialOrdersDf$US_State)
partialOrdersDf$Age <- as.numeric(partialOrdersDf$Age)

summary(partialOrdersDf)
```

# Summary Statistics
## Clicks
```{r summarytable1, echo=FALSE}

table_data <- clickstream_data %>%
  select(Request_Processing_Time, Request_Sequence, REQUEST_HOUR_OF_DAY) %>%
  transmute(mean_RPT = mean(Request_Processing_Time), mean_RS = mean(Request_Sequence), mean_HOD = mean(REQUEST_HOUR_OF_DAY),
            sd_RPT = sd(Request_Processing_Time), sd_RS = sd(Request_Sequence), sd_HOD = sd(REQUEST_HOUR_OF_DAY),
            min_RPT = min(Request_Processing_Time), min_RS = min(Request_Sequence), min_HOD = min(REQUEST_HOUR_OF_DAY),
            max_RPT = max(Request_Processing_Time), max_RS = max(Request_Sequence), max_HOD = max(REQUEST_HOUR_OF_DAY))

table_data <- table_data[1,] %>%
  gather("column", "mean", 1:3) %>%
  gather("sd_data", "sd", 1:3) %>%
  gather("min_data", "min", 1:3) %>%
  gather("max_data", "max", 1:3) %>%
  mutate(column = sapply(strsplit(column, "_"), "[", 2),
         sd_data = sapply(strsplit(sd_data, "_"), "[", 2),
         min_data = sapply(strsplit(min_data, "_"), "[", 2),
         max_data = sapply(strsplit(max_data, "_"), "[", 2)) %>%
  filter(column == sd_data & column == min_data & column == max_data) %>%
  select(column, mean, sd, min, max)

kable(table_data)
```
## Customers

It is always important to know who your customers are when conducting a business. Therefore we looked at some of the most apparent properties of your customers. First we looked on how old ebuy's customers are. Your youngest customer is 18 years old while your oldest customer indeed is 98 years old. Although it is nice to see that there are some very old people who know how to operate a computer properly for ebuy it is more intersting to see that your average customer is in his/her mid 30's. Maybe ebuy should research what trends are emerging in this age segment so you can fit your offerings to your audience.

```{r ageDistro, echo = FALSE}
filtered = orderDfCleaned %>% 
  na_if("?")  %>% 
  distinct(Customer_ID, .keep_all = TRUE)

summary(as.numeric(as.character(filtered$Age)))

```

When looking at the gender distribution in your customer base it is interesting to observe that most of your customers are women. Maybe ebuy should consider putting some marketing in place to attract more men to the platform or fully commit to only sell products for women.

```{r genderDistro, echo = FALSE}
filtered = orderDfCleaned %>%  na_if("NULL")  %>% distinct(Customer_ID, .keep_all = TRUE)
summary(as.factor(as.character(filtered$Gender)))

```

Furthermore we was interested into how people get to know about ebuy and it turns out that most of ebuy's customers got aware of ebuy through they friends and family in the first place. It seems like ebuy is really dependent on mouth to mouth propaganda. E-mail advertisements are also working quite fine in generating new leads, whereas only a fraction of customers got interested into ebuy via print ads. Maybe ebuy should look closer into social media marketing and other types of modern marketing mechanisms if you want to get more independent from mouth to mouth propaganda which is hard to control although it is working fine for now.

```{r howDidYouHearAboutUs, echo = FALSE}
filtered = orderDfCleaned %>%  na_if("?")  %>% distinct(Customer_ID, .keep_all = TRUE)
summary(as.factor(as.character(filtered$HowDidYouHearAboutUs)))

```

We also tried to get insight about how many of your customers are parents. If the share is high maybe it makes sense to also offer products for babies and young children since parents buy the clothing for their kinds until a specific age of the children. Maybe parents will just put a new pyjama for their kid in the basket while shopping a new dress for themselve. It turns out that indeed approximatively 40% of your  customers are parents.

```{r presenceOfChildren, echo = FALSE}
filtered = orderDfCleaned %>% na_if("?")  %>% distinct(Customer_ID, .keep_all = TRUE)
summary(as.factor(as.logical(filtered$Presence_Of_Children)))

```

## Products
For ebuy it should be interesting to find out, which of their products and product families are the ones that sell best. The best selling product is a pair of socks from brand HOSO, their best selling product family has the ID 12295. Unfortunately the data does not provide any meaningful further details, what this product family is made of.
```{r mostOrderedProductsAndProductFamilies, echo=FALSE}
findDistinctBrandname <- function(dataFrame, productId) {
  bn <- dataFrame %>%
    filter(Product_ID == productId) %>%
    select(BrandName) %>%
    distinct(BrandName) %>%
    slice(1:1)
  return(bn$BrandName)
}

topSellingProducts <- partialOrdersDf %>%
  select(Product_ID, BrandName) %>%
  drop_na(Product_ID) %>%
  count(Product_ID) %>%
  arrange(desc(n)) %>%
  rowwise() %>%
  mutate(
    brandName = findDistinctBrandname(partialOrdersDf, Product_ID)
  )

topSellingProducts %>%
  slice(1:5) %>%
  kable(.) %>%
  kable_styling(full_width=FALSE)

partialOrdersDf %>%
  drop_na(Product_Family_ID) %>%
  count(Product_Family_ID) %>%
  arrange(desc(n)) %>%
  slice(1:5) %>%
  kable(.) %>%
  kable_styling(full_width=FALSE)
```


# Plots
## Clicks
```{r heatmap, echo=FALSE}


map_data <- clickstream_data %>%
  select(US_State) %>%
  group_by(US_State) %>%
  summarise(Request_Count = n(), state = first(US_State))

plot_usmap(data = map_data, values = "Request_Count", labels = TRUE) +
  scale_fill_continuous(low = "white", high = "red", name = "average Order Amount") +
  labs(title = "How much money do customers from different states spend?",
       subtitle = "Average order amount split by US State") +
  theme(plot.title = element_text(face = "bold", size = 15, hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5, size = 12, face = "italic"),
        legend.position = "bottom", legend.justification = "center",
        legend.background = element_blank())

```

```{r requests, echo = FALSE}

data_requests_products <- clickstream_data %>%
  select(Product_ID) %>%
  filter(Product_ID != "?") %>%
  group_by(Product_ID) %>%
  summarise(Request_Count = n())

ggplot(data_requests_products) +
  geom_line(aes(x = reorder(Product_ID, desc(Request_Count)), y = Request_Count, group = 1)) +
  labs(x = "Products", y = "Requests",
       title = "How many times was each product accessed?",
       subtitle = "Requests per product")  +
  theme_classic() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank(),
        plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5)) +
  scale_y_continuous(expand = c(0,0))

```

```{r sessionduration, echo=FALSE}
  duration_data <- clickstream_data %>%
  select(Session_ID, Request_Sequence, Request_Date, Request_Date_Time, Session_First_Request_Date, Session_First_Request_Date_Time) %>%
  unite(col = "Request_Timestamp",c("Request_Date", "Request_Date_Time"), sep = " ", remove = TRUE) %>%
  unite(col="Session_First_Request_Timestamp", c("Session_First_Request_Date","Session_First_Request_Date_Time"), sep=" ", remove=TRUE) %>%
  mutate(Request_Timestamp = as.POSIXct(Request_Timestamp, format="%Y-%m-%d %H\\:%M\\:%S")) %>%
  mutate(Session_First_Request_Timestamp = as.POSIXct(Session_First_Request_Timestamp, format="%Y-%m-%d %H\\:%M\\:%S")) %>%
  group_by(Session_ID) %>%
  summarise(Pages_Visited=n(),Session_First_Request=min(Session_First_Request_Timestamp),Session_Last_Request=max(Request_Timestamp)) %>%
  mutate(Session_Duration = as.numeric(difftime(Session_Last_Request, Session_First_Request,units = "secs",))) %>%
  group_by(Pages_Visited) %>%
  summarise(avg_Session_Duration = mean(Session_Duration), Session_Count = n_distinct(Session_ID)) %>%
  filter(Session_Count > 2 & Pages_Visited > 1)
  
  ggplot(data = duration_data) +
  geom_point(aes(x = Pages_Visited, y = avg_Session_Duration, size = Session_Count)) +
  labs(x = "Pages Visited", y = "average Session Duration (seconds)", size = "Sessions",
       title = "How long do customers stay on the site?",
       subtitle = "Average session duration per session page visits") +
  theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
```

## Customers

### How is the sales volume distributed among customers?

We wanted to know if there is maybe a little group of customers which is responsible for large parts of the overall spendings on ebuy. If this is the case it is probably important to focus on such a group as they are generating most of your revenue. Therefore we looked at how the spendings on ebuy are distributed among the customers. The lorenz curve in figure \@ref(fig:calculateMoneySpendPerCustomer) vizualizes this distribution. 
As you can see the curve shows that the distribution of sales is not equally distributed among the customers. The curve's gini coefficient has a value of 0.46 indicating an very unequal distribution. Interestingly the upper ten percent of customers are responsible for approximately 42% of total sales. This tells us that indeed there is a  customer group which spends much more on ebuy than the remaining majority. We suggest that ebuy should look take attention on this group and foster their willingness to spend money on ebuy, maybe with exclusive offerings for royality customers.

```{r calculateMoneySpendPerCustomer, echo=FALSE, fig.cap="Lorenz Curve - Customer Spendings"}
money_spent_per_customer <- orderDfCleaned %>%
  group_by(Customer_ID) %>%
  summarise(Total_Spending = sum(Order_Line_Amount))

lorenz_money_spent_per_customer <- Lc(money_spent_per_customer$Total_Spending)
lorenz_money_spent_per_customer_df <- data.frame(p=lorenz_money_spent_per_customer$p, L=lorenz_money_spent_per_customer$L)

ggplot(data=lorenz_money_spent_per_customer_df) +
  geom_line(aes(x=p, y=L)) +
  scale_x_continuous(name="Cumulative share of Customers", limits=c(0,1)) + 
  scale_y_continuous(name="Cumulative share of Spendings", limits=c(0,1)) +
  geom_abline(color = "grey")
```

### How are customer spendings distributed among the different brands?

We also looked into whether people are spending their money equally for all brands or if spendings are focussed on a particular group of brands. The lorenz curve in figure \@ref(fig:lorenzBrandToSales) has a gini coefficient of 0.36 indicating a quite inequal distribution of sales to the respective brands. The upper 10 percent of brands are responsible for approximately 28% of generated sales while the lower 50 percent of the brands are only responsible for 21% of sales. Maybe ebuy should consider to boost some of the very successful brands because people tend to buy more of it. Ebuy could show new customers products of one of the successful brands when they first visit the ebuy page because they will more likely buy it which will in turn increase sales. In the following we will look into which are the most successful brands for men and women. 

```{r lorenzBrandToSales, echo = FALSE, fig.cap="Lorenz Curve - Brand Spendings" }
orders_with_brand <- orderDfCleaned
orders_with_brand$Brand = unlist(lapply(orderDfCleaned$Product_Level_2_Path, FUN = function(path){return(str_match(path, "\\/.*\\/.*\\/(.*)")[,2])}))

money_spent_per_brand <- orders_with_brand %>%
  group_by(Brand) %>%
  summarise(Total_Spending = mean(Order_Line_Amount))


lorenz_money_spent_per_brand <- Lc(money_spent_per_brand$Total_Spending)

lorenz_money_spent_per_brand_df <- data.frame(p=lorenz_money_spent_per_brand$p, L=lorenz_money_spent_per_brand$L)

ggplot(data=lorenz_money_spent_per_brand_df) +
  geom_line(aes(x=p, y=L)) +
  scale_x_continuous(name="Cumulative share of Brands", limits=c(0,1)) + 
  scale_y_continuous(name="Cumulative share of Spendings", limits=c(0,1)) +
  geom_abline(color = "grey")
```

### What are the most popular brands for men and women?

```{r include=FALSE}
orders = orderDfCleaned %>%
  select(Customer_ID, Order_Line_Session_ID, Order_ID, Customer_ID,
         Age, Gender, Product_Level_2_Path, Order_Status, Order_Line_Quantity)
orders$Brand = unlist(lapply(orders$Product_Level_2_Path, FUN = function(path){return(str_match(path, "\\/.*\\/.*\\/(.*)")[,2])}))
```

To get an overview over what brands your customers like the most we vizualized the popularity of the brands based on how many articles was bought from the brand. Hanes and American Essentials are the most popular brands among ebuy's female customers (compare figure \@ref(fig:brandsFemale)). Interestingly those are also the two most successful brands for men as you can see in figure \@ref(fig:brandsMale)) although men bought products from American Essential twice as often as from Hanes.

```{r prepareBrandData, include= FALSE}
mostPopularFemaleBrands = orders %>% 
  filter(Gender == "Female") %>%
  filter(!is.na(Brand)) %>%
  group_by(Brand) %>%
  summarise(Order_Amount = sum(Order_Line_Quantity)) %>%
  arrange(desc(Order_Amount)) %>%
  head(10)
```

```{r brandsFemale, echo=FALSE, fig.cap= "Most Popular Brands for Females (based on cumulated order amount)"}
ggplot(mostPopularFemaleBrands, mapping = aes(x = reorder(Brand, Order_Amount), y = Order_Amount, fill = Brand)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Brand", y = "Order Amount")
```

```{r include=FALSE}
mostPopularMaleBrands = orders %>%
  filter(Gender == "Male") %>%
  filter(!is.na(Brand)) %>%
  group_by(Brand) %>%
  summarise(Order_Amount = sum(Order_Line_Quantity)) %>%
  arrange(desc(Order_Amount)) %>%
  head(10)
```

```{r brandsMale, echo=FALSE, fig.cap="Most Popular Brands for Males (based on cumulated order amount)"}
ggplot(mostPopularMaleBrands, mapping = aes(x = reorder(Brand, Order_Amount), y = Order_Amount, fill = Brand)) + 
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Brand", y = "Number of Purchases")
```

## Orders
### Different order volumes on different days of the week?
Orders at ebuy's online shop might be distributed differently among the different days of the week. To examine, whether this is the case, we plotted a frequency distribution of all orders over the days of week. We first reordered the days in the canonical way (i.e. Monday, Tuesday, Wednesday, ...) and then counted and plotted the order frequencies against them (figure \@ref(fig:orderDayBarPlot1)). Clearly, the most orders are placed during the weekdays (peaking on Wednesdays), the least orders on sunday. In order to ramp up their sales on those days where sales are particulary low (Saturday, Sunday, Monday), ebuy could for instance try to hand out (digital) coupons that are only valid on the weekend, therby hopefully increasing their sales figures.
```{r orderDayBarPlot1, echo=FALSE, fig.cap="Distribution of orders among different days of the week"}
partialOrdersDf %>%
  mutate(
    Order_Day_of_Week = factor(Order_Day_of_Week, levels=c(
      "Monday",
      "Tuesday",
      "Wednesday",
      "Thursday",
      "Friday",
      "Saturday",
      "Sunday"))
    ) %>%
  count(Order_Day_of_Week) %>%
  slice(1:7) %>%
  ggplot(data = .) +
    geom_bar(mapping = aes(x = Order_Day_of_Week, y = n), stat = "identity")
```

### What is the distribution of order amounts?
For an online shop like ebuy it might be of interest to identify those customers that place the vast majority of all orders. In order to visualize the distribution of order amounts we computed their Lorenz curve and plotted the cummulative percentages (figure \@ref(fig:orderTotalLc)).
```{r orderTotalLc, echo=FALSE, fig.cap="Lorenz Curve: Distribution of order amounts"}
lc <- Lc(partialOrdersDf$Order_Amount,
         n = rep(1, length(partialOrdersDf$Order_Amount)),
         plot = F)

p <- lc[1]
L <- lc[2]
lc <- data.frame(p,L)

ggplot(data = lc) +
  geom_line(aes(x = p, y = L)) +
  scale_y_continuous(name="% of order amounts", limits=c(0,1)) +
  scale_x_continuous(name="% of orders", limits=c(0,1)) +
  geom_abline()
```


# Ebuy's Experiment
## Data and Summary Statistics
Ebuy's experiment provided us with data regarding different recommender systems. After loading and examining the dataset we picked all columns of interest (e.g. deciding to keep the first ID column only) and recoded the chosen ID column as a factor. We than calculated basic summary statistics, where the mean values where of most interest.
```{r loadexpdata, echo=FALSE}
experimentDf <- read.csv(file="../experiment/experimental_results.csv", sep=",", na.strings=c("NA"))
experimentDf <- experimentDf[c("id_", "ranking_based", "random_recommendations", "profit_oriented")]
names(experimentDf)[1] <- "id"
experimentDf$id <- as.factor(experimentDf$id)
summary(experimentDf)
```

## Broad Overview of Distributions

When taking a broad look on the means and densities of the different recommender systems, you can clearly see that the profit-oriented recommender system has the highest mean sales with an average of 21.75€ per customer (see figure \@ref(fig:profitDensityAndMean)) in contrast to an average of  19€ per customer for the ranking-based recommender (\@ref(fig:rankingDensityAndMean)) and 18.50€ for the random recommender (see figure \@ref(fig:randomDensityAndMean)). It is also worth to mention that the density for the profit-based recommender system is pretty narrow (compare figure  \@ref(fig:profitDensityAndMean)), indicating a lower standard deviation and probably a tighter confidence interval than the other two wider distributions for the ranking-based (see figure \@ref(fig:rankingDensityAndMean)) and the random recommender systems (see figure \@ref(fig:randomDensityAndMean)).

```{r recommenderSetup, include=FALSE}
recommender_to_sales <- setNames(data.frame(matrix(ncol = 3, nrow = 0)), c("Id", "Recommender", "Sales"))
index <- 1
for(row in 1:nrow(experimentDf)){
  if(!is.na(experimentDf[row, "ranking_based"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "ranking_based",
                                                                as.numeric(experimentDf[row, "ranking_based"]))
    index <- index+1
  }
  if(!is.na(experimentDf[row, "random_recommendations"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "random_recommendations",
                                                                as.numeric(experimentDf[row, "random_recommendations"]))
    index <- index+1
  }
  if(!is.na(experimentDf[row, "profit_oriented"])){
    recommender_to_sales[nrow(recommender_to_sales) +1,] = list(index, "profit_oriented",
                                                                as.numeric(experimentDf[row, "profit_oriented"]))
    index <- index+1
  }
}
recommender_to_sales$Id <- NULL
```

```{r rankingDensityAndMean, echo=FALSE, fig.cap="Ranking-Based Recommender - Sales Density and Mean"}
ggplot(recommender_to_sales %>% filter(Recommender=="ranking_based"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density")
```

```{r profitDensityAndMean, echo=FALSE, fig.cap="Profit-Oriented Recommender - Sales Density and Mean"}
ggplot(recommender_to_sales %>% filter(Recommender=="profit_oriented"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density")
```

```{r randomDensityAndMean, echo=FALSE, fig.cap="Random Recommender - Sales Density and Mean"}
ggplot(recommender_to_sales %>% filter(Recommender=="random_recommendations"), aes(x=Sales)) +
  stat_density(geom="line") +
  geom_vline(aes(xintercept=mean(Sales)),color="black", linetype="dashed", size=1) +
  labs(x="Sales",y= "Density")
```

When taking a closer look at the confidence intervals of the different recommender systems (see figure \@ref(fig:confidenceIntervalsRecommenders)) you can clearly  see that the profit-based recommender systems shows the best performance and also has a very tight 95% confidence interval indicating high certainty in the amount of profit made from the recommender per customer. On the other side the confidence intervals for the profit-based recommender and the random recommender system are more wide and also overlap each other.

```{r confidenceIntervalsRecommenders, echo=FALSE, fig.cap="Recommender Confidence Intervals"}
result <- group_by(recommender_to_sales, Recommender) %>%
  summarise(
    Count = n(),
    Mean = mean(Sales, na.rm = TRUE),
    Sd = sd(Sales, na.rm = TRUE),
    Se = Sd/sqrt(Count),
    CiMult = qt(0.975, Count-1),
    Ci = Se * CiMult
  )

# visualize CI
ggplot(result, aes(x=Recommender, y=Mean, group=1)) +
  geom_point(alpha=0.52) +
  geom_errorbar(width=.1, aes(ymin=Mean-Ci, ymax=Mean+Ci), colour="darkred") + labs(x="Recommender System",y= "Sales")
```

Therefore we should definitely do a statisitcal test to prove if the means of the different recommender systems are significantly different from each other or not.

## Shapiro-Wilk Normality Test

Before conducting a t-test to test if the profit means of the different recommender systems are significantly different from each other or not we will first check if their populations are normally distributed, which is a necessary precondition for conducting a t-test. In order to do so we used the Shapiro-Wilk normality test which tests the null-hypothesis that the population is normally distributed.

```{r echo = FALSE}
with(recommender_to_sales, shapiro.test(Sales[Recommender == "profit_oriented"]))
with(recommender_to_sales, shapiro.test(Sales[Recommender == "ranking_based"]))
with(recommender_to_sales, shapiro.test(Sales[Recommender == "random_recommendations"]))
```
It turns out that all the p-values are smaller than the significance level of 0.05 which means that we have to refuse the null-hypothesis which in turn implies that the distributions are not normally distributed. Therefore we used the Unpaired Two-Samples Wilcoxon Test (aka Mann-Whitney U Test) to test if the means of the different recommenders are equal or not. The Mann-Whitney U Test is an valid alternative to the t-test in szenarios in which the requirements for the t-test are not met and is widely used in practice.

## Mann-Whitney U Test

```{r echo = FALSE}
# perform wulxoc test
ranking_based_sales <- recommender_to_sales %>% filter(Recommender == "ranking_based")
profit_oriented_sales <- recommender_to_sales %>% filter(Recommender == "profit_oriented")
random_recommendations_sales <- recommender_to_sales %>% filter(Recommender == "random_recommendations")

wilcox.test(ranking_based_sales$Sales, random_recommendations_sales$Sales, alternative = "two.sided")
wilcox.test(profit_oriented_sales$Sales, random_recommendations_sales$Sales, alternative = "two.sided")
```

We first tested if there is a significant difference in the means of the random recommender system and the ranking-based recommender system. Our earlier observations of the confidence intervals of the two recommenders revealed that their confidence intervals are both quite wide and also overlap at large parts (see figure \@ref(fig:confidenceIntervalsRecommenders)). The test now proves that in fact the both means of the two recommenders do not differ significantly (p = 0.2008 > 0.05).
Although the confidence intervals in figure \@ref(fig:confidenceIntervalsRecommenders) already indicate that the means of the profit-based recommender is significantly different from the random recommender system we conducted a test for that. The result of the test proves our suggestions with an p-value less than 0.001.
As the mean of the ranking-based recommender system has been shown to not differ significantly from the random recommendersystem we have to choose between the profit-oriented recommender system and the random recommender system. As the profit-oriented recommender systems mean value (21.75) is greater than the baseline mean (17.91), ebuy should preferably use the profit-oriented one.

# Models and Prediction
```{r includes02, echo=FALSE, message=FALSE}
library(caret)
library(rpart)
library(rpart.plot)
```

## Training the Model{#subsection-ttm}
As ebuy's products fall into certain product families, we trained a model that allows to predict for a new customer from which product family he will purchase a product. We first grew a large tree with a complexity parameter of cp=0.00001 (i.e. splits had to decrease the lack of fit by a factor of 0.00001). We then examined the results of a 10-fold cross-validation by plotting the complexity parameter vs. its cross-validated error (as shown in figure \@ref(fig:cpplottree1)).
```{r cpplottree1, echo=FALSE, fig.cap="Product Family ID: Complexity parameter vs. cross-validated error"}
tree <- rpart(Product_Family_ID ~ Order_Line_Day_of_Week + Gender + Age,
              method="class",
              data=partialOrdersDf,
              cp=0.00001)

plotcp(tree)
```

To prevent over-fitting, @kabacoff2015 suggests to choose the leftmost cp value below the dotted line (here: cp=0.0007): a tree of that size (here: 45 splits) is the smallest tree whose cross-validated error is whithin one standard error of the minimum cross-validated error value. We therfore pruned the tree to its new size and pretty-printed the resulting tree (figure \@ref(fig:prunedtree1)).
```{r prunedtree1, echo=FALSE, fig.cap="Product Family ID: Pruned Classification Tree"}
prune(tree, cp=0.0007) %>%
  prp(.)
```

Next we computed a 10-fold cross-validated linear regression model that helped us understand, how the different properties of customers affected the amount of money they would spend in total.
```{r linReg1, echo=FALSE}
ctrl <- trainControl(method = "cv",
                     number = 10)

linreg1 <- train(Order_Line_Amount ~ Age + Gender,
                 data = partialOrdersDf,
                 trControl = ctrl,
                 na.action  = na.pass,
                 method = "lm")
linreg1
linreg1$finalModel
```

## Predictions
Having trained the models in \@ref(subsection-ttm) ebuy could predict the purchasing behaviour of future customers. Given such a previously unseen customer, ebuy could for instance now predict the product family which she is going to purchase from.
```{r prediction1, eval=FALSE}
predict(tree, unseenCustomer, type="class")
```


# Summary

# Literature
